{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD  # For dimensionality reduction\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and split the data (English to Hindi dataset)\n",
        "def load_data(file_name):\n",
        "    data = pd.read_excel(file_name)\n",
        "    return data['ENGLISH'], data['HINDI']\n",
        "\n",
        "def split_data(X, y, test_size=0.2, random_state=42):\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "def train_random_forest(X_train, y_train, n_estimators=10):  # Reduce the number of trees\n",
        "    # Training a RandomForestClassifier\n",
        "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# SHAP Explanation and Feature Importance\n",
        "def explain_model_with_shap(model, X_train, feature_names):\n",
        "    # Initialize SHAP explainer for tree-based models (Random Forest in this case)\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "\n",
        "    # Calculate SHAP values for the training set\n",
        "    shap_values = explainer.shap_values(X_train)\n",
        "\n",
        "    # Plot the summary of feature importance\n",
        "    shap.summary_plot(shap_values, X_train, feature_names=feature_names)\n",
        "\n",
        "def main():\n",
        "    file_name = \"Book1.xlsx\"\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    english_text, hindi_text = load_data(file_name)\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "    # Initialize the TF-IDF vectorizer with a max_features limit\n",
        "    print(\"Initializing TF-IDF vectorizer...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit the number of features\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(english_text)\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    print(\"TF-IDF transformation completed.\")\n",
        "\n",
        "    # Apply dimensionality reduction (PCA or SVD)\n",
        "    svd = TruncatedSVD(n_components=100)\n",
        "    X_reduced = svd.fit_transform(X_tfidf)\n",
        "\n",
        "    # Split the data\n",
        "    print(\"Splitting data into training and testing sets...\")\n",
        "    X_train, X_test, y_train, y_test = split_data(X_reduced, hindi_text)\n",
        "    print(f\"Data split completed. Training set size: {X_train.shape[0]}, Testing set size: {X_test.shape[0]}.\")\n",
        "\n",
        "    # Train the RandomForest model\n",
        "    print(\"Training Random Forest model...\")\n",
        "    model = train_random_forest(X_train, y_train)\n",
        "    print(\"Model training completed.\")\n",
        "\n",
        "    # Explain the model with SHAP and plot feature importances\n",
        "    print(\"Explaining the model with SHAP...\")\n",
        "    explain_model_with_shap(model, X_train, feature_names)\n",
        "    print(\"SHAP explanation completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFSVuJ6FDkfM",
        "outputId": "4701b442-02a6-4c1a-e25f-3a56c76b5436"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data loaded successfully.\n",
            "Initializing TF-IDF vectorizer...\n",
            "TF-IDF transformation completed.\n",
            "Splitting data into training and testing sets...\n",
            "Data split completed. Training set size: 2448, Testing set size: 613.\n",
            "Training Random Forest model...\n",
            "Model training completed.\n",
            "Explaining the model with SHAP...\n"
          ]
        }
      ]
    }
  ]
}