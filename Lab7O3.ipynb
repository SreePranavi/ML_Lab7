{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import time\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Load the English and Hindi text data from the specified Excel file.\"\"\"\n",
        "    data = pd.read_excel(file_name)\n",
        "    return data['ENGLISH'], data['HINDI']\n",
        "\n",
        "def remove_rare_classes(X, y, min_samples=3):\n",
        "    \"\"\"Remove classes from the dataset that have fewer than the specified minimum samples.\"\"\"\n",
        "    value_counts = y.value_counts()\n",
        "    valid_classes = value_counts[value_counts >= min_samples].index\n",
        "    mask = y.isin(valid_classes)\n",
        "    return X[mask], y[mask]\n",
        "\n",
        "def split_data(X, y, test_size=0.3, random_state=42):\n",
        "    \"\"\"Split the data into training and testing sets.\"\"\"\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
        "\n",
        "def grid_search_cv(X_train, y_train, n_splits=2):\n",
        "    \"\"\"Perform GridSearchCV to find the best hyperparameters for the Random Forest model.\"\"\"\n",
        "    # Define the parameter grid for hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [10, 50, 100],\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    rf = RandomForestClassifier(class_weight='balanced')\n",
        "    cv = StratifiedKFold(n_splits=n_splits)\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=0)\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "\n",
        "    return grid_search.best_params_, grid_search.best_score_, end_time - start_time\n",
        "\n",
        "def randomized_search_cv(X_train, y_train, n_splits=2):\n",
        "    \"\"\"Perform RandomizedSearchCV to find the best hyperparameters for the Random Forest model.\"\"\"\n",
        "    # Define the parameter distribution for hyperparameter tuning\n",
        "    param_dist = {\n",
        "        'n_estimators': [10, 50, 100, 200],\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    rf = RandomForestClassifier(class_weight='balanced')\n",
        "    cv = StratifiedKFold(n_splits=n_splits)\n",
        "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=10, cv=cv, n_jobs=-1, verbose=0, random_state=42)\n",
        "    start_time = time.time()\n",
        "    random_search.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "\n",
        "    return random_search.best_params_, random_search.best_score_, end_time - start_time\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to load data, preprocess, and perform hyperparameter tuning.\"\"\"\n",
        "    file_name = \"Book1.xlsx\"\n",
        "\n",
        "    english_text, hindi_text = load_data(file_name)\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(english_text)\n",
        "\n",
        "    svd = TruncatedSVD(n_components=100)\n",
        "    X_reduced = svd.fit_transform(X_tfidf)\n",
        "\n",
        "    X_reduced, hindi_text = remove_rare_classes(X_reduced, hindi_text, min_samples=3)\n",
        "    X_train, X_test, y_train, y_test = split_data(X_reduced, hindi_text, test_size=0.3)\n",
        "\n",
        "    # Run GridSearchCV and capture best parameters and performance score\n",
        "    grid_params, grid_score, grid_time = grid_search_cv(X_train, y_train, n_splits=2)\n",
        "    # Run RandomizedSearchCV and capture best parameters and performance score\n",
        "    random_params, random_score, random_time = randomized_search_cv(X_train, y_train, n_splits=2)\n",
        "\n",
        "    # Print performance comparison results\n",
        "    print(\"\\nPerformance Comparison:\")\n",
        "    print(f\"GridSearchCV Best Score: {grid_score:.4f}, Time Taken: {grid_time:.4f} seconds\")\n",
        "    print(f\"RandomizedSearchCV Best Score: {random_score:.4f}, Time Taken: {random_time:.4f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reCIPim8jazn",
        "outputId": "95851910-bf4d-405e-8052-158882f40a48"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Comparison:\n",
            "GridSearchCV Best Score: 0.8944, Time Taken: 5.7969 seconds\n",
            "RandomizedSearchCV Best Score: 0.9444, Time Taken: 2.4508 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}